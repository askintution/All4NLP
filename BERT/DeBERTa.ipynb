{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torch import _softmax_backward_data as _softmax_backward_data\n",
    "import numpy as np\n",
    "\n",
    "import copy\n",
    "from collections.abc import Sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_relative_position(query_size, key_size):\n",
    "    \"\"\" Build relative position according to the query and key\n",
    "\n",
    "    We assume the absolute position of query :math:`P_q` is range from \n",
    "    (0, query_size) and the absolute position of key :math:`P_k` is range from (0, key_size),\n",
    "    The relative positions from query to key is\n",
    "    \n",
    "    :math:`R_{q \\\\rightarrow k} = P_q - P_k`\n",
    "\n",
    "    Args:\n",
    "        query_size (int): the length of query\n",
    "        key_size (int): the length of key\n",
    "\n",
    "    Return:\n",
    "        :obj:`torch.LongTensor`: A tensor with shape [1, query_size, key_size]\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    q_ids = np.arange(0, query_size)\n",
    "    k_ids = np.arange(0, key_size)\n",
    "    rel_pos_ids = q_ids[:, None] - np.tile(k_ids, (q_ids.shape[0],1))\n",
    "    rel_pos_ids = torch.tensor(rel_pos_ids, dtype=torch.long)\n",
    "    rel_pos_ids = rel_pos_ids[:query_size, :]\n",
    "    rel_pos_ids = rel_pos_ids.unsqueeze(0)\n",
    "    return rel_pos_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayerNorm(nn.Module):\n",
    "  \"\"\"LayerNorm module in the TF style (epsilon inside the square root).\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, size, eps=1e-12):\n",
    "    super().__init__()\n",
    "    self.weight = nn.Parameter(torch.ones(size))\n",
    "    self.bias = nn.Parameter(torch.zeros(size))\n",
    "    self.variance_epsilon = eps\n",
    "\n",
    "  def forward(self, x):\n",
    "    input_type = x.dtype\n",
    "    x = x.float()\n",
    "    u = x.mean(-1, keepdim=True)\n",
    "    s = (x - u).pow(2).mean(-1, keepdim=True)\n",
    "    x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n",
    "    x = x.to(input_type)\n",
    "    y = self.weight * x + self.bias\n",
    "    return y\n",
    "class BertSelfOutput(nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "    self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "    self.LayerNorm = BertLayerNorm(config.hidden_size, config.layer_norm_eps)\n",
    "    self.dropout = StableDropout(config.hidden_dropout_prob)\n",
    "    self.config = config\n",
    "\n",
    "  def forward(self, hidden_states, input_states, mask=None):\n",
    "    hidden_states = self.dense(hidden_states)\n",
    "    hidden_states = self.dropout(hidden_states)\n",
    "    hidden_states += input_states\n",
    "    hidden_states = MaskedLayerNorm(self.LayerNorm, hidden_states)\n",
    "    return hidden_states\n",
    "\n",
    "class BertAttention(nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "    self.self = DisentangledSelfAttention(config)\n",
    "    self.output = BertSelfOutput(config)\n",
    "    self.config = config\n",
    "\n",
    "  def forward(self, hidden_states, attention_mask, \n",
    "              return_att=False, query_states=None, relative_pos=None, rel_embeddings=None):\n",
    "    self_output = self.self(hidden_states, attention_mask, return_att, \n",
    "                            query_states=query_states, relative_pos=relative_pos, rel_embeddings=rel_embeddings)\n",
    "    if return_att:\n",
    "      self_output, att_matrix = self_output\n",
    "    if query_states is None:\n",
    "      query_states = hidden_states\n",
    "    attention_output = self.output(self_output, query_states, attention_mask)\n",
    "\n",
    "    if return_att:\n",
    "      return (attention_output, att_matrix)\n",
    "    else:\n",
    "      return attention_output\n",
    "\n",
    "class BertIntermediate(nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "    self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "    self.intermediate_act_fn = ACT2FN[config.hidden_act] \\\n",
    "      if isinstance(config.hidden_act, str) else config.hidden_act\n",
    "\n",
    "  def forward(self, hidden_states):\n",
    "    hidden_states = self.dense(hidden_states)\n",
    "    hidden_states = self.intermediate_act_fn(hidden_states)\n",
    "    return hidden_states\n",
    "\n",
    "class BertOutput(nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super(BertOutput, self).__init__()\n",
    "    self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "    self.LayerNorm = BertLayerNorm(config.hidden_size, config.layer_norm_eps)\n",
    "    self.dropout = StableDropout(config.hidden_dropout_prob)\n",
    "    self.config = config\n",
    "\n",
    "  def forward(self, hidden_states, input_states, mask=None):\n",
    "    hidden_states = self.dense(hidden_states)\n",
    "    hidden_states = self.dropout(hidden_states)\n",
    "    hidden_states += input_states\n",
    "    hidden_states = MaskedLayerNorm(self.LayerNorm, hidden_states)\n",
    "    return hidden_states\n",
    "\n",
    "class BertLayer(nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super(BertLayer, self).__init__()\n",
    "    self.attention = BertAttention(config)\n",
    "    self.intermediate = BertIntermediate(config)\n",
    "    self.output = BertOutput(config)\n",
    "\n",
    "  def forward(self, hidden_states, attention_mask, return_att=False, \n",
    "              query_states=None, relative_pos=None, rel_embeddings=None):\n",
    "    attention_output = self.attention(hidden_states, attention_mask, return_att=return_att, \\\n",
    "      query_states=query_states, relative_pos=relative_pos, rel_embeddings=rel_embeddings)\n",
    "    if return_att:\n",
    "      attention_output, att_matrix = attention_output\n",
    "    intermediate_output = self.intermediate(attention_output)\n",
    "    layer_output = self.output(intermediate_output, attention_output, attention_mask)\n",
    "    if return_att:\n",
    "      return (layer_output, att_matrix)\n",
    "    else:\n",
    "      return layer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu(x):\n",
    "  \"\"\"Implementation of the gelu activation function.\n",
    "    For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n",
    "    0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "  \"\"\"\n",
    "  return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
    "\n",
    "\n",
    "def swish(x):\n",
    "  return x * torch.sigmoid(x)\n",
    "\n",
    "def linear_act(x):\n",
    "  return x\n",
    "\n",
    "ACT2FN = {\"gelu\": gelu, \"relu\": torch.nn.functional.relu, \n",
    "          \"swish\": swish, \"tanh\": torch.nn.functional.tanh, \n",
    "          \"linear\": linear_act, 'sigmoid': torch.sigmoid}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelConfig:\n",
    "    def __init__(self):        \n",
    "        self.hidden_size = 768\n",
    "        self.num_hidden_layers = 12\n",
    "        self.num_attention_heads = 12\n",
    "        self.hidden_act = \"gelu\"\n",
    "        self.intermediate_size = 3072\n",
    "        self.hidden_dropout_prob = 0.1\n",
    "        self.attention_probs_dropout_prob = 0.1\n",
    "        self.max_position_embeddings = 512\n",
    "        self.type_vocab_size = 0\n",
    "        self.initializer_range = 0.02\n",
    "        self.layer_norm_eps = 1e-7\n",
    "        self.padding_idx = 0\n",
    "        self.vocab_size = 21128\n",
    "        \n",
    "        self.relative_attention = True\n",
    "        self.max_relative_positions = 512\n",
    "        self.position_biased_input = True\n",
    "        self.pos_att_type = \"p2c|c2p\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ModelConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 与 Bert 一致\n",
    "class BertEmbeddings(nn.Module):\n",
    "  \"\"\"Construct the embeddings from word, position and token_type embeddings.\n",
    "  \"\"\"\n",
    "  def __init__(self, config):\n",
    "    super(BertEmbeddings, self).__init__()\n",
    "    padding_idx = getattr(config, 'padding_idx', 0)\n",
    "    self.embedding_size = getattr(config, 'embedding_size', config.hidden_size)\n",
    "    self.word_embeddings = nn.Embedding(config.vocab_size, self.embedding_size, padding_idx = padding_idx)\n",
    "\n",
    "    self.position_biased_input = getattr(config, 'position_biased_input', True)\n",
    "    if not self.position_biased_input:\n",
    "      self.position_embeddings = None\n",
    "    else:\n",
    "      self.position_embeddings = nn.Embedding(config.max_position_embeddings, self.embedding_size)\n",
    "\n",
    "    if config.type_vocab_size>0:\n",
    "      self.token_type_embeddings = nn.Embedding(config.type_vocab_size, self.embedding_size)\n",
    "    \n",
    "    if self.embedding_size != config.hidden_size:\n",
    "      self.embed_proj = nn.Linear(self.embedding_size, config.hidden_size, bias=False)\n",
    "    self.LayerNorm = BertLayerNorm(config.hidden_size, config.layer_norm_eps)\n",
    "    self.dropout = StableDropout(config.hidden_dropout_prob)\n",
    "    self.output_to_half = False\n",
    "    self.config = config\n",
    "\n",
    "  def forward(self, input_ids, token_type_ids=None, position_ids=None, mask = None):\n",
    "    seq_length = input_ids.size(1)\n",
    "    if position_ids is None:\n",
    "      position_ids = torch.arange(0, seq_length, dtype=torch.long, device=input_ids.device)\n",
    "      position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "    if token_type_ids is None:\n",
    "      token_type_ids = torch.zeros_like(input_ids)\n",
    "\n",
    "    words_embeddings = self.word_embeddings(input_ids)\n",
    "    print(\"words_embeddings shape: \", words_embeddings.shape)\n",
    "    if self.position_embeddings is not None:\n",
    "      position_embeddings = self.position_embeddings(position_ids.long())\n",
    "      print(\"position_embeddings shape: \", position_embeddings.shape)\n",
    "    else:\n",
    "      position_embeddings = torch.zeros_like(words_embeddings)\n",
    "\n",
    "    embeddings = words_embeddings\n",
    "    if self.position_biased_input:\n",
    "      embeddings += position_embeddings\n",
    "    if self.config.type_vocab_size>0:\n",
    "      token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
    "      embeddings += token_type_embeddings\n",
    "\n",
    "    if self.embedding_size != self.config.hidden_size:\n",
    "      embeddings = self.embed_proj(embeddings)\n",
    "\n",
    "    embeddings = MaskedLayerNorm(self.LayerNorm, embeddings, mask)\n",
    "    embeddings = self.dropout(embeddings)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([[31, 51, 99, 10, 9, 8, 7, 6], [15, 5, 4, 3, 2, 1, 0, 0]])\n",
    "attention_mask = torch.tensor([[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 0, 0]])\n",
    "token_type_ids = torch.zeros_like(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertEmbeddings(\n",
       "  (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "  (position_embeddings): Embedding(512, 768)\n",
       "  (LayerNorm): BertLayerNorm()\n",
       "  (dropout): StableDropout()\n",
       ")"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = BertEmbeddings(config)\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words_embeddings shape:  torch.Size([2, 8, 768])\n",
      "position_embeddings shape:  torch.Size([2, 8, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 768])"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_output = embeddings(input_ids, token_type_ids, mask=attention_mask)\n",
    "embedding_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distengled Attentin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attention_mask(attention_mask):\n",
    "    extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "    att_mask = extended_attention_mask.byte()\n",
    "    attention_mask = att_mask*att_mask.squeeze(-2).unsqueeze(-1)\n",
    "    return attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 0, 0]])"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1]]],\n",
       "\n",
       "\n",
       "        [[[1, 1, 1, 1, 1, 1, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0]]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_attention_mask(attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rel_pos(hidden_states, relative_pos=None):\n",
    "    q = hidden_states.size(-2)\n",
    "    relative_pos = build_relative_position(q, hidden_states.size(-2))\n",
    "    return relative_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0, -1, -2, -3, -4, -5, -6, -7],\n",
       "         [ 1,  0, -1, -2, -3, -4, -5, -6],\n",
       "         [ 2,  1,  0, -1, -2, -3, -4, -5],\n",
       "         [ 3,  2,  1,  0, -1, -2, -3, -4],\n",
       "         [ 4,  3,  2,  1,  0, -1, -2, -3],\n",
       "         [ 5,  4,  3,  2,  1,  0, -1, -2],\n",
       "         [ 6,  5,  4,  3,  2,  1,  0, -1],\n",
       "         [ 7,  6,  5,  4,  3,  2,  1,  0]]])"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_rel_pos(embedding_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0, -1, -2, -3, -4, -5, -6, -7],\n",
       "         [ 1,  0, -1, -2, -3, -4, -5, -6],\n",
       "         [ 2,  1,  0, -1, -2, -3, -4, -5],\n",
       "         [ 3,  2,  1,  0, -1, -2, -3, -4],\n",
       "         [ 4,  3,  2,  1,  0, -1, -2, -3],\n",
       "         [ 5,  4,  3,  2,  1,  0, -1, -2],\n",
       "         [ 6,  5,  4,  3,  2,  1,  0, -1],\n",
       "         [ 7,  6,  5,  4,  3,  2,  1,  0]]])"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_relative_position(8, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 8])"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8])"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        layer = BertLayer(config)\n",
    "        self.layer = nn.ModuleList([copy.deepcopy(layer) for _ in range(config.num_hidden_layers)])\n",
    "        self.relative_attention = getattr(config, 'relative_attention', False)\n",
    "        self.max_relative_positions = getattr(config, 'max_relative_positions', -1)\n",
    "        self.rel_embeddings = nn.Embedding(self.max_relative_positions*2, config.hidden_size)\n",
    "\n",
    "    def get_attention_mask(self, attention_mask):\n",
    "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "        att_mask = extended_attention_mask.byte()\n",
    "        attention_mask = att_mask*att_mask.squeeze(-2).unsqueeze(-1)\n",
    "        return attention_mask\n",
    "\n",
    "    def get_rel_pos(self, hidden_states, query_states=None, relative_pos=None):\n",
    "        q = hidden_states.size(-2)\n",
    "        relative_pos = build_relative_position(q, hidden_states.size(-2))\n",
    "        return relative_pos\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask, \n",
    "                output_all_encoded_layers=True, return_att=True,\n",
    "                query_states = None, relative_pos=None):\n",
    "        \n",
    "        attention_mask = self.get_attention_mask(attention_mask)\n",
    "        relative_pos = self.get_rel_pos(hidden_states, query_states, relative_pos)\n",
    "\n",
    "        all_encoder_layers = []\n",
    "        att_matrixs = []\n",
    "        next_kv = hidden_states\n",
    "        rel_embeddings = self.rel_embeddings.weight\n",
    "        \n",
    "        for i, layer_module in enumerate(self.layer):\n",
    "            output_states = layer_module(next_kv, \n",
    "                                         attention_mask, \n",
    "                                         return_att, \n",
    "                                         query_states = query_states, \n",
    "                                         relative_pos=relative_pos, \n",
    "                                         rel_embeddings=rel_embeddings)\n",
    "            output_states, att_m = output_states\n",
    "            next_kv = output_states\n",
    "            all_encoder_layers.append(output_states)\n",
    "            att_matrixs.append(att_m)\n",
    "        return (all_encoder_layers, att_matrixs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "encoder = BertEncoder(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_layers = encoder(embedding_output, attention_mask, output_all_encoded_layers=True, return_att=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_encoder_layers, att_matrixs = encoded_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_encoder_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 768])"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_encoder_layers[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(att_matrixs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 12, 8, 8])"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_matrixs[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = DisentangledSelfAttention(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DisentangledSelfAttention(\n",
       "  (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
       "  (pos_dropout): StableDropout()\n",
       "  (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "  (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (dropout): StableDropout()\n",
       ")"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 768])"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 8, 8])"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_attention_mask(attention_mask).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_embeddings = nn.Embedding(config.max_relative_positions*2, config.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "(context_layer, attention_probs) = attn(embedding_output, \n",
    "     get_attention_mask(attention_mask), \n",
    "     return_att=True, query_states=None, \n",
    "     relative_pos=build_relative_position(embedding_output.size(-2), embedding_output.size(-2)), # seq_len\n",
    "     rel_embeddings=rel_embeddings.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 768])"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_layer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 12, 8, 8])"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8])"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 8, 8])"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_attention_mask(attention_mask).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_relative_position(embedding_output.size(-2), embedding_output.size(-2)).dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 8, 8])"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_relative_position(embedding_output.size(-2), embedding_output.size(-2)).unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StableDropout()"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_dropout = StableDropout(config.hidden_dropout_prob)\n",
    "pos_dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 768])"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_dropout(rel_embeddings.weight).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transpose_for_scores(x):\n",
    "    new_x_shape = x.size()[:-1] + (config.num_attention_heads, -1)\n",
    "    x = x.view(*new_x_shape)\n",
    "    return x.permute(0, 2, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(torch.Size([2, 8, 2304]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 12, 8, 192])"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transpose_for_scores(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "q,k,v = transpose_for_scores(x).chunk(3, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 12, 8, 64]),\n",
       " torch.Size([2, 12, 8, 64]),\n",
       " torch.Size([2, 12, 8, 64]))"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 1, 64])"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_bias = torch.nn.Parameter(torch.zeros((768), dtype=torch.float))\n",
    "transpose_for_scores(q_bias.unsqueeze(0).unsqueeze(0)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.max_relative_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 768])"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisentangledSelfAttention(torch.nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads) # 768/12 = 64\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size # 12 * 64 = 768\n",
    "        self.in_proj = torch.nn.Linear(config.hidden_size, self.all_head_size*3, bias=False) # 768, 768*3\n",
    "        self.q_bias = torch.nn.Parameter(torch.zeros((self.all_head_size), dtype=torch.float))\n",
    "        self.v_bias = torch.nn.Parameter(torch.zeros((self.all_head_size), dtype=torch.float))\n",
    "        self.pos_att_type = [\"p2c\", \"c2p\"]\n",
    "\n",
    "        self.max_relative_positions = config.max_relative_positions\n",
    "        self.pos_dropout = StableDropout(config.hidden_dropout_prob)\n",
    "\n",
    "        self.pos_proj = torch.nn.Linear(config.hidden_size, self.all_head_size, bias=False)\n",
    "        self.pos_q_proj = torch.nn.Linear(config.hidden_size, self.all_head_size)\n",
    "\n",
    "        self.dropout = StableDropout(config.attention_probs_dropout_prob)\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, -1)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask, \n",
    "                return_att=False, query_states=None, \n",
    "                relative_pos=None, rel_embeddings=None):\n",
    "        \"\"\"  Call the module\n",
    "        Args:\n",
    "            hidden_states (:obj:`torch.FloatTensor`):\n",
    "                Input states to the module usally the output from previous layer, \n",
    "                it will be the Q,K and V in `Attention(Q,K,V)`\n",
    "\n",
    "            attention_mask (:obj:`torch.ByteTensor`):\n",
    "                An attention mask matrix of shape [`B`, `N`, `N`] where `B` is the batch size, \n",
    "                `N` is the maxium sequence length in which element [i,j] = `1` means \n",
    "                the `i` th token in the input can attend to the `j` th token.\n",
    "\n",
    "            return_att (:obj:`bool`, optional):\n",
    "                Whether return the attention maxitrix.\n",
    "\n",
    "            query_states (:obj:`torch.FloatTensor`, optional):\n",
    "                The `Q` state in `Attention(Q,K,V)`.\n",
    "\n",
    "            relative_pos (:obj:`torch.LongTensor`):\n",
    "                The relative position encoding between the tokens in the sequence. \n",
    "                It's of shape [`B`, `N`, `N`] with values ranging in [`-max_relative_positions`, \n",
    "                `max_relative_positions`].\n",
    "\n",
    "            rel_embeddings (:obj:`torch.FloatTensor`):\n",
    "                The embedding of relative distances. \n",
    "                It's a tensor of shape [:math:`2 \\\\times \\\\text{max_relative_positions}`, `hidden_size`].\n",
    "        \"\"\"\n",
    "        # (batch_size, seq_len, hidden_size * 3)\n",
    "        qp = self.in_proj(hidden_states)\n",
    "        # (batch_size, num_attention_heads, seq_len, seq_len * attention_head_size).chunk(3, dim=-1) =>\n",
    "        # (batch_size, num_attention_heads, seq_len, attention_head_size)\n",
    "        query_layer,key_layer, value_layer = self.transpose_for_scores(qp).chunk(3, dim=-1)\n",
    "        \n",
    "        query_layer += self.transpose_for_scores(self.q_bias.unsqueeze(0).unsqueeze(0))\n",
    "        value_layer += self.transpose_for_scores(self.v_bias.unsqueeze(0).unsqueeze(0))\n",
    "\n",
    "        rel_att = None\n",
    "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
    "        scale_factor = 1\n",
    "        if 'c2p' in self.pos_att_type:\n",
    "            scale_factor += 1\n",
    "        if 'p2c' in self.pos_att_type:\n",
    "            scale_factor += 1\n",
    "        if 'p2p' in self.pos_att_type:\n",
    "            scale_factor += 1\n",
    "        scale = math.sqrt(query_layer.size(-1)*scale_factor)\n",
    "        query_layer = query_layer/scale\n",
    "        # (batch_size, num_attention_heads, query_size, key_size)\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "        \n",
    "        # 本文定义的额外计算 Attention 分数\n",
    "        rel_embeddings = self.pos_dropout(rel_embeddings)\n",
    "        # (batch_size, num_attention_heads, query_size, key_size)\n",
    "        rel_att = self.disentangled_att_bias(query_layer, key_layer, \n",
    "                                             relative_pos, rel_embeddings, \n",
    "                                             scale_factor)\n",
    "\n",
    "        attention_scores = attention_scores + rel_att\n",
    "        \n",
    "        attention_probs = XSoftmax.apply(attention_scores, attention_mask, -1)\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (-1,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "        \n",
    "        return (context_layer, attention_probs)\n",
    "\n",
    "    def disentangled_att_bias(self, query_layer, key_layer, relative_pos, rel_embeddings, scale_factor):\n",
    "        # query_layer: (batch_size, num_attention_heads, query_seq_len, attention_head_size)\n",
    "        # key_layer: like query_layer\n",
    "        # relative_pos: (1, query_size, key_size)\n",
    "        # rel_embeddings: (max_relative_positions*2, hidden_size)\n",
    "        # scale_factor: 3\n",
    "        \n",
    "        # relative_pos.dim()==3:\n",
    "        # (1, query_size, key_size) => (1, 1, query_size, key_size)\n",
    "        relative_pos = relative_pos.unsqueeze(1)\n",
    "\n",
    "        # int number\n",
    "        att_span = min(max(query_layer.size(-2), key_layer.size(-2)), self.max_relative_positions)\n",
    "        relative_pos = relative_pos.long().to(query_layer.device)\n",
    "        # (1, att_span*2, hidden_size)\n",
    "        # 层间共享的 P\n",
    "        rel_embeddings = rel_embeddings[self.max_relative_positions - att_span:\n",
    "                                        self.max_relative_positions + att_span, :].unsqueeze(0)\n",
    "        \n",
    "        # 位置 Kr\n",
    "        if 'c2p' in self.pos_att_type:\n",
    "            # without bias\n",
    "            # (1, att_span*2, hidden_size)\n",
    "            pos_key_layer = self.pos_proj(rel_embeddings)\n",
    "            # (1, num_attention_heads, att_span*2, attention_head_size)\n",
    "            pos_key_layer = self.transpose_for_scores(pos_key_layer)\n",
    "        # 位置 Qr\n",
    "        if 'p2c' in self.pos_att_type:\n",
    "            # with bias\n",
    "            # (1, att_span*2, hidden_size)\n",
    "            pos_query_layer = self.pos_q_proj(rel_embeddings)\n",
    "            # (1, num_attention_heads, att_span*2, attention_head_size)\n",
    "            pos_query_layer = self.transpose_for_scores(pos_query_layer)\n",
    "\n",
    "        score = 0\n",
    "        # content->position\n",
    "        if 'c2p' in self.pos_att_type:\n",
    "            # (batch_size, num_attention_heads, query_size, att_span * 2)\n",
    "            c2p_att = torch.matmul(query_layer, pos_key_layer.transpose(-1, -2))\n",
    "            # (1, 1, query_size, key_size)  # i-j+k, [0, 2*k)\n",
    "            c2p_pos = torch.clamp(relative_pos + att_span, 0, att_span*2-1)\n",
    "            # (batch_size, num_attention_heads, query_size, key_size)\n",
    "            # expand(batch_size, num_attention_heads, query_size, key_size)\n",
    "            c2p_att = torch.gather(c2p_att, dim=-1, index=c2p_pos.expand(\n",
    "                [query_layer.size(0), query_layer.size(1), query_layer.size(2), relative_pos.size(-1)]))\n",
    "            score += c2p_att\n",
    "\n",
    "        # position->content\n",
    "        if 'p2c' in self.pos_att_type:\n",
    "            pos_query_layer /= math.sqrt(pos_query_layer.size(-1)*scale_factor)\n",
    "            # j-i+k, [0, 2*k), δ(j,i)\n",
    "            p2c_pos = torch.clamp(-relative_pos + att_span, 0, att_span*2-1)\n",
    "            # (batch_size, num_attention_heads, key_size, att_span * 2)\n",
    "            p2c_att = torch.matmul(key_layer, pos_query_layer.transpose(-1, -2))\n",
    "            # expand(batch_size, num_attention_heads, key_size, query_size)\n",
    "            # transpose to => (batch_size, num_attention_heads, query_size, key_size)\n",
    "            p2c_att = torch.gather(p2c_att, dim=-1, index=p2c_pos.expand(\n",
    "                [key_layer.size(0), key_layer.size(1), key_layer.size(2), relative_pos.size(-2)])\n",
    "                                  ).transpose(-1, -2)\n",
    "            # expand 里面稍微改了一下，以前是这样的：\n",
    "            # [query_layer.size(0), query_layer.size(1), key_layer.size(-2), key_layer.size(-2)]\n",
    "            score += p2c_att\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "attn = DisentangledSelfAttention(config)\n",
    "(context_layer, attention_probs) = attn(embedding_output, \n",
    "     get_attention_mask(attention_mask), \n",
    "     return_att=True, query_states=None, \n",
    "     relative_pos=build_relative_position(embedding_output.size(-2), embedding_output.size(-2)), # seq_len\n",
    "     rel_embeddings=rel_embeddings.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 8, 768]), torch.Size([2, 12, 8, 8]))"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_layer.shape, attention_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 3, 3])"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_relative_position(embedding_output.size(-2), embedding_output.size(-2)).unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0, -1, -2],\n",
       "          [ 1,  0, -1],\n",
       "          [ 2,  1,  0]]]])"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = build_relative_position(embedding_output.size(-2), embedding_output.size(-2)).unsqueeze(1)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 12, 3, 3])"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.expand([2, 12, 3, 3]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.gather?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([[1,2],[3,4]])\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1],\n",
       "        [4, 3]])"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.gather(t, -1, torch.tensor([[0,0],[1,0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
